{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSINTSY - Machine Learning Model\n",
    "### Title: Machine Learning Analysis on Crop Recommendation\n",
    "\n",
    "### Members: Cruzada, Escalona, Francisco, Loyola\n",
    "\n",
    "### Section and Group CSINTSY S14 MCO5\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Different crops can be produced with different amounts of nitrogen, phosphorus, potassium in soil, and varying measures of temperature, humidity, soil reaction (pH), and rainfall. \n",
    "\n",
    "Nitrogren is responsible for increasing the yield and quantity of crops. When the nitrogren rate is increased, so does its yield. Furthermore, phosphorus and potassium have roles in maintaining the crop. Phosphorus is the major component in the RNA and DNA of a plant and plays a role in root developmnt, crop maturity, and seed production while Potassium increases water use efficiency and transforms sugars to starch in the grain-filling process. In addition, Potassium allows plants to withstand extreme hot and cold temperatures, drought, and pests. \n",
    "\n",
    "Temperature in the context of crop production is the measure of intensity of heat energy. As a result, it influences the distribution of crop plants and vegetation. As for the humidity, it influences the water requirement of crops. Soil reaction (pH) is the hydrogen ion concentration of the soil. This affects crop growth as a low pH interferes with the availability of other plant nutrients. Finally, rainfall is one of the most important factors that influences the crop production. Different crops grow in heavy and low rainfall areas. However, it is important to note that yields are not always directly proportional to the amount of rainfall.\n",
    "\n",
    "### Motivation \n",
    "It is important to be informed about the different crops that can be produced with varying levels of different factors mentioned above to maximize crop production and agricultural productivity. If these were not identified, the time, money, and effort invested into growing a certain crop could be wasted because of an unfulfilled condition in cultivating the said crop. Hence, the question to be answered using **K-Nearest Neighbor Algorithm** and **Decision Tree** is:\n",
    "\n",
    "#### *What crops will be produced given the different levels of nitrogen, phosphorus, potassium in soil, temperature, humidity, pH, and rainfall?*\n",
    "\n",
    "### Benefits\n",
    "**<< Then, enumerate or explain the possible benefits if the problem is solved. >>**\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset was obtained through the data science and dataset sharing platform of Kaggle. It was uploaded by a user named Abhishek Kumar who also had other datasets aside from the crops recommendation dataset. The dimensions of the dataset contains 8 columns with 2200 rows which meets the requirment of the dataset having a minimum of at least 1000 datapoints.\n",
    "\n",
    "The features of the dataset are environmental parameters to which the crop is ideal to grow in to. These features are as follows:\n",
    "\n",
    "- **`Nitrogen levels`** :  Represents the amount of nitrogen (in kg/ha) present in the soil for the crop\n",
    "- **`Phosphorus levels`** : Represents the amount of phosphorus (in kg/ha) present in the soil for the crop\n",
    "- **`Potassium levels`** :  Represents the amount of potassium (in kg/ha) present in the soil for the crop\n",
    "- **`Temperature`** : Represents the average temperature (in Celsius) during the crop's growing period\n",
    "- **`Humidity levels`** : Represents the average relative humidity (in percentage) during the crop's growing period\n",
    "- **`pH levels`** :  Represents the soil pH during the crop's growing period\n",
    "- **`Rainfall levels`** : Represents the amount of rainfall (in mm) received during the crop's growing period\n",
    "\n",
    "The combination of these features results to certain 'environments' that a certain kind of crop may be suitable to grow in, thus each datapoint is equally labelled with a crop that is deemed to grow well at that kind of environment. The labels of these crops are as follows:\n",
    "1. Rice\n",
    "2. Maize\n",
    "3. Chickpea\n",
    "4. Kidneybeans\n",
    "5. Pigeonpeas\n",
    "6. Mothbeans\n",
    "7. Mungbean\n",
    "8. Blackgram\n",
    "9. Lentil\n",
    "10. Pomegranate\n",
    "11. Banana\n",
    "12. Mango\n",
    "13. Grapes\n",
    "14. Watermelon\n",
    "15. Muskmelon\n",
    "16. Apple\n",
    "17. Orange\n",
    "18. Papaya\n",
    "19. Coconut\n",
    "20. Cotton\n",
    "21. Jute\n",
    "22. Coffee\n",
    "\n",
    "The dataset will be divided into two segments for use which are for training and testing data. The ratio between the training and testing data will be 2000:200 from the original 2200. The test data will be randomly selected through the use of a seeded random selector for consistency on every runtime.\n",
    "\n",
    "## ML Methods Used\n",
    "\n",
    "The machine learning methods used are ones that are capable of multi-label classification techniques. This is for the reason that the dataset labels the different combination of environmental parameters to an appropriate type of crop of different kinds (i.e., multiple labels). Among the techniques used are the following:\n",
    "1. K-Nearest Neighbor Classifier\n",
    "2. Decision Tree Classifier\n",
    "\n",
    "## References\n",
    "\n",
    "- https://www.noble.org/news/publications/ag-news-and-views/2007/january/back-to-basics-the-roles-of-n-p-k-and-their-sources/\n",
    "- http://eagri.org/eagri50/AGRO101/lec09.pdf\n",
    "- https://www.kaggle.com/datasets/aksahaha/crop-recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Proper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing and cleaning data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Raw Data\n",
    "\n",
    "The training data shows 8 unique columns which represent certain parameters. With the exception of the `label` parameter, these parameters are the environmental variables that are required in the growth of a certain crop which are the levels of `nitrogen`, `phosphorus`, `potassium`, `temperature` (in C), `humidity`, `ph`, and `rainfall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nitrogen</th>\n",
       "      <th>phosphorus</th>\n",
       "      <th>potassium</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ph</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>20.879744</td>\n",
       "      <td>82.002744</td>\n",
       "      <td>6.502985</td>\n",
       "      <td>202.935536</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>21.770462</td>\n",
       "      <td>80.319644</td>\n",
       "      <td>7.038096</td>\n",
       "      <td>226.655537</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>23.004459</td>\n",
       "      <td>82.320763</td>\n",
       "      <td>7.840207</td>\n",
       "      <td>263.964248</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>26.491096</td>\n",
       "      <td>80.158363</td>\n",
       "      <td>6.980401</td>\n",
       "      <td>242.864034</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>20.130175</td>\n",
       "      <td>81.604873</td>\n",
       "      <td>7.628473</td>\n",
       "      <td>262.717340</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>107</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>26.774637</td>\n",
       "      <td>66.413269</td>\n",
       "      <td>6.780064</td>\n",
       "      <td>177.774507</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>99</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>27.417112</td>\n",
       "      <td>56.636362</td>\n",
       "      <td>6.086922</td>\n",
       "      <td>127.924610</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>118</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>24.131797</td>\n",
       "      <td>67.225123</td>\n",
       "      <td>6.362608</td>\n",
       "      <td>173.322839</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>117</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>26.272418</td>\n",
       "      <td>52.127394</td>\n",
       "      <td>6.758793</td>\n",
       "      <td>127.175293</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>104</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>23.603016</td>\n",
       "      <td>60.396475</td>\n",
       "      <td>6.779833</td>\n",
       "      <td>140.937041</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nitrogen  phosphorus  potassium  temperature   humidity        ph  \\\n",
       "0           90          42         43    20.879744  82.002744  6.502985   \n",
       "1           85          58         41    21.770462  80.319644  7.038096   \n",
       "2           60          55         44    23.004459  82.320763  7.840207   \n",
       "3           74          35         40    26.491096  80.158363  6.980401   \n",
       "4           78          42         42    20.130175  81.604873  7.628473   \n",
       "...        ...         ...        ...          ...        ...       ...   \n",
       "2195       107          34         32    26.774637  66.413269  6.780064   \n",
       "2196        99          15         27    27.417112  56.636362  6.086922   \n",
       "2197       118          33         30    24.131797  67.225123  6.362608   \n",
       "2198       117          32         34    26.272418  52.127394  6.758793   \n",
       "2199       104          18         30    23.603016  60.396475  6.779833   \n",
       "\n",
       "        rainfall   label  \n",
       "0     202.935536    rice  \n",
       "1     226.655537    rice  \n",
       "2     263.964248    rice  \n",
       "3     242.864034    rice  \n",
       "4     262.717340    rice  \n",
       "...          ...     ...  \n",
       "2195  177.774507  coffee  \n",
       "2196  127.924610  coffee  \n",
       "2197  173.322839  coffee  \n",
       "2198  127.175293  coffee  \n",
       "2199  140.937041  coffee  \n",
       "\n",
       "[2200 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Training data\n",
    "crops_df = pd.read_csv('crops_dataset.csv')\n",
    "crops_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Check for duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_df.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Check for incorrect datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Determine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 90.        ,  42.        ,  43.        , ...,  82.00274423,\n",
       "          6.50298529, 202.9355362 ],\n",
       "       [ 85.        ,  58.        ,  41.        , ...,  80.31964408,\n",
       "          7.03809636, 226.6555374 ],\n",
       "       [ 60.        ,  55.        ,  44.        , ...,  82.3207629 ,\n",
       "          7.84020714, 263.9642476 ],\n",
       "       ...,\n",
       "       [118.        ,  33.        ,  30.        , ...,  67.22512329,\n",
       "          6.36260785, 173.3228386 ],\n",
       "       [117.        ,  32.        ,  34.        , ...,  52.12739421,\n",
       "          6.75879255, 127.1752928 ],\n",
       "       [104.        ,  18.        ,  30.        , ...,  60.39647474,\n",
       "          6.77983261, 140.9370415 ]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['nitrogen', 'phosphorus', 'potassium', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "X = crops_df[features].to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Determine Unique Crop Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique crop labels:  22\n",
      "['rice' 'maize' 'chickpea' 'kidneybeans' 'pigeonpeas' 'mothbeans'\n",
      " 'mungbean' 'blackgram' 'lentil' 'pomegranate' 'banana' 'mango' 'grapes'\n",
      " 'watermelon' 'muskmelon' 'apple' 'orange' 'papaya' 'coconut' 'cotton'\n",
      " 'jute' 'coffee']\n"
     ]
    }
   ],
   "source": [
    "#Determine the list of unique crops\n",
    "y = crops_df['label'].to_numpy()\n",
    "labels = crops_df[\"label\"].unique()\n",
    "print(\"No. of unique crop labels: \", labels.size)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Divide Training Data from Test Data\n",
    "\n",
    "With a original dataset having 2200 rows or data points, it is possible to retrive at most 200 test samples to be used for testing with the remaining 2000 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=.11, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Visualization\n",
    "\n",
    "This section shows the relationships of certain pairs or groups of features that is found in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Nitrogen, Phosphorus, Potassium Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppr_graph = plt.figure()\n",
    "nppr_graph_ax = plt.axes(projection='3d')\n",
    "nppr_graph_ax.scatter3D(crops_df['nitrogen'],crops_df['phosphorus'], crops_df['potassium'])\n",
    "nppr_graph_ax.set_xlabel('Nitrogen')\n",
    "nppr_graph_ax.set_ylabel('Phosphorus')\n",
    "nppr_graph_ax.set_zlabel('Potassium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Temperature, Humidity, Rainfall Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrr_graph = plt.figure()\n",
    "thrr_graph_ax = plt.axes(projection='3d')\n",
    "thrr_graph_ax.scatter3D(crops_df['temperature'],crops_df['humidity'], crops_df['rainfall'])\n",
    "thrr_graph_ax.set_xlabel('Temperature')\n",
    "thrr_graph_ax.set_ylabel('Humidity')\n",
    "thrr_graph_ax.set_zlabel('Rainfall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Temperature, Humidity, pH Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thpr_graph = plt.figure()\n",
    "thrp_graph_ax = plt.axes(projection='3d')\n",
    "thrp_graph_ax.scatter3D(crops_df['temperature'],crops_df['humidity'], crops_df['ph'])\n",
    "thrp_graph_ax.set_xlabel('Temperature')\n",
    "thrp_graph_ax.set_ylabel('Humidity')\n",
    "thrp_graph_ax.set_zlabel('pH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-Nearest Neighbor (KNN) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Load and 'Train' K Neighbors Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3) #Uses number of labels\n",
    "knn.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9835"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_accuracy = round(accuracy_score(y_test, y_pred),4)\n",
    "knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing different of neighbors, the rate of correct predictions is still around >=95% accurate for K value that is less than 100. The table below contains the different values for neighbors and its respective accuracy rate.\n",
    "\n",
    "| K Value | Accuracy Rate |   |   |   |\n",
    "|---------|---------------|---|---|---|\n",
    "| 1       | 0.97          |   |   |   |\n",
    "| 2       | 0.99          |   |   |   |\n",
    "| 3       | 0.99          |   |   |   |\n",
    "| 10      | 0.98          |   |   |   |\n",
    "| 15      | 0.97          |   |   |   |\n",
    "| 20      | 0.97          |   |   |   |\n",
    "| 22      | 0.97          |   |   |   |\n",
    "| 100     | 0.95          |   |   |   |\n",
    "| 200     | 0.79          |   |   |   |\n",
    "| 300     | 0.64          |   |   |   |\n",
    "| 400     | 0.64          |   |   |   |\n",
    "| 500     | 0.39          |   |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Load and 'Train' Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dct = DecisionTreeClassifier()\n",
    "dct.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Plot Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "plot_tree(dct)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dct.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5 Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_accuracy = round(accuracy_score(y_test, y_pred),4)\n",
    "dt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Load and 'Train' Logistic Regression Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-840024e5aaf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mSAGA\u001b[0m \u001b[0msolver\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mfloat64\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfloat32\u001b[0m \u001b[0mbit\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \"\"\"\n\u001b[1;32m-> 1306\u001b[1;33m         \u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_check_solver\u001b[1;34m(solver, penalty, dual)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[0mall_penalties\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'l1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'elasticnet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_penalties\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         raise ValueError(\"Logistic Regression supports only penalties in %s,\"\n\u001b[0m\u001b[0;32m    440\u001b[0m                          \" got %s.\" % (all_penalties, penalty))\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None."
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(penalty=None, verbose=True, max_iter=10000, n_jobs=os.cpu_count())\n",
    "lr_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Test Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_accuracy = round(accuracy_score(y_test, y_pred),4)\n",
    "lr_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparison of Machine Learning Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the resulting accuracies of the different ML models used in this experiment which are:\n",
    "1. K-Nearest Neighbor\n",
    "2. Decision Tree\n",
    "3. Logistic Regression \n",
    "\n",
    "respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-b176ceab5b07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'K-Nearest Neighbor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknn_accuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Decision Tree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_accuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Logistic Regression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_accuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"ML Classifier Model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dt_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "raw_accuracy = [['K-Nearest Neighbor', knn_accuracy],['Decision Tree', dt_accuracy],['Logistic Regression', lr_accuracy]]\n",
    "\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "data = np.array(raw_accuracy)\n",
    "columns = (\"ML Classifier Model\", \"Accuracy\")\n",
    "axs.axis('tight')\n",
    "axs.axis('off')\n",
    "the_table = axs.table(cellText=data, colLabels=columns, loc='center', cellLoc='center')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Folds Cross Validation comparision for ML Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "KNN Accuracy: 0.985\n",
      "DCT Accuracy: 0.99\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-bdd75a7ec7a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"DCT Accuracy: {round(accuracy_score(test_set_labels, dct_pred),4)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_set_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mlr_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"LR Accuracy: {round(accuracy_score(test_set_labels, lr_pred),4)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mSAGA\u001b[0m \u001b[0msolver\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mfloat64\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfloat32\u001b[0m \u001b[0mbit\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \"\"\"\n\u001b[1;32m-> 1306\u001b[1;33m         \u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_check_solver\u001b[1;34m(solver, penalty, dual)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[0mall_penalties\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'l1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'elasticnet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_penalties\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         raise ValueError(\"Logistic Regression supports only penalties in %s,\"\n\u001b[0m\u001b[0;32m    440\u001b[0m                          \" got %s.\" % (all_penalties, penalty))\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=11, shuffle=True)\n",
    "knn = KNeighborsClassifier(n_neighbors=3) #Uses number of labels\n",
    "dct = DecisionTreeClassifier()\n",
    "lr_model = LogisticRegression(penalty=None, verbose=True, max_iter=10000, n_jobs=os.cpu_count())\n",
    "\n",
    "# kf will return list of indices for the train_index and test_index\n",
    "for i, (train_index, test_index) in enumerate(kf.split(crops_df)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    axis = 0 \n",
    "    # Extract from the dataframe based on the given indeces\n",
    "    training_set = np.take(X, train_index, axis)\n",
    "    training_set_labels = np.take(y, train_index, axis)\n",
    "    test_set = np.take(X, test_index, axis)\n",
    "    test_set_labels = np.take(y, test_index, axis)\n",
    "\n",
    "    knn.fit(training_set,training_set_labels)\n",
    "    knn_pred = knn.predict(test_set)\n",
    "    print(f\"KNN Accuracy: {round(accuracy_score(test_set_labels, knn_pred),4)}\")\n",
    "    \n",
    "    dct.fit(training_set,training_set_labels)\n",
    "    dct_pred = dct.predict(test_set)\n",
    "    print(f\"DCT Accuracy: {round(accuracy_score(test_set_labels, dct_pred),4)}\")\n",
    "    \n",
    "    # lr_model.fit(training_set,training_set_labels)\n",
    "    # lr_pred = lr_model.predict(test_set)\n",
    "    # print(f\"LR Accuracy: {round(accuracy_score(test_set_labels, lr_pred),4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
